{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab04. Web Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook mainly goes over how to get data with the Python packages `requests` and  `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec0'></a>\n",
    "## Pre-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a pseudo-module which programmers can use to enable new language features which are not compatible with the current interpreter. For example, the expression 11 over 4 (11/4) currently evaluates to 2. If the module in which it is executed had enabled true division by executing. The expression 11/4 would evaluate to 2.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1'></a>\n",
    "# Webscraping intro\n",
    "\n",
    "In order to scrape content from a website we first need to download the HTML contents of the website. This can be done with the Python library **requests** (with its `.get` method).\n",
    "\n",
    "Then when we want to extract certain information from a website we use the scraping tool **BeautifulSoup4** (import bs4). In order to extract information with beautifulsoup we have to create a soup object from the HTML source code of a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests # The requests library is an \n",
    "# HTTP library for getting content and posting etc.\n",
    "\n",
    "import bs4 as bs # BeautifulSoup4 is a Python library \n",
    "# for pulling data out of HTML and XML code.\n",
    "# we can query markup languages for specific content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping a simple website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = requests.get(\"http://www.comp.hkbu.edu.hk/~hugolee/\") \n",
    "# a GET request will download the HTML webpage.\n",
    "\n",
    "print(source) # If <Response [200]> then \n",
    "# the website has been downloaded succesfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different types of repsonses:**\n",
    "Generally status code starting with 2 indicates success. Status code starting with 4 or 5 indicates error. Frequent appearance of the status codes like 404 (Not Found), 403 (Forbidden), 408 (Request Timeout) might indicate that you got blocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(source.content) # This is the HTML content of the website,\n",
    "# as you can see it's quite hard to decipher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert source.content to a beautifulsoup object \n",
    "# beautifulsoup can parse (extract specific information) HTML code\n",
    "\n",
    "soup = bs.BeautifulSoup(source.content, features='html.parser') \n",
    "# we pass in the source content\n",
    "# features specifies what type of code we are parsing, \n",
    "# here 'html.parser' specifies that we want beautiful soup to parse HTML code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(soup) # looks a lot nicer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(soup.prettify()) \n",
    "# .prettify() method makes the HTML code more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we printed the HTML code of the website, decoded as a beautiful soup object.\n",
    "\n",
    "### HTML tags\n",
    "`<xxx> </xxx>`: are all the HTML tags, that specifies certain sections, stylings etc of the website, for more info: \n",
    "https://www.w3schools.com/tags/ref_byfunc.asp\n",
    "\n",
    "## **class and id: ** \n",
    "\n",
    "class and id attributes of HTML tags, they are used as hooks to give unique styling to certain elements and an id for sections / parts of the page.\n",
    "\n",
    "- **id:** is a unique tag for a specific element (this often does not change)\n",
    "- **class:** specifies a class of objects. Several elements in the HTML code can have the same class.\n",
    "\n",
    "Full list of HTML tags: https://developer.mozilla.org/en-US/docs/Web/HTML/Element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose we want to extract content that is shown on the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inside the <body> tag of the website is where all the main content is\n",
    "print(soup.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.title) # Title of the website\n",
    "print(soup.find('title')) # same as .title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If we want to extract specific text\n",
    "print(soup.find('p')) # will only return first <p> tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(soup.find('p').text) # extracts the string within the <p> tag, strips it of tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for p in soup.find_all('p'): # print all text paragraphs on the webpage\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract links / urls\n",
    "# Links in html is usually coded as <a href=\"url\">\n",
    "# where the link is url\n",
    "\n",
    "print(soup.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup.a.get('href') \n",
    "# to get the link from href attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if we want to list links and their text info\n",
    "\n",
    "links = soup.find_all('a')\n",
    "\n",
    "for l in links:\n",
    "    print(\"Info about {}: \".format(l.text), \\\n",
    "          l.get('href')) \n",
    "# then we have extracted the link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find table:  \n",
    "Usually organized data in HTML format on a website is stored in tables under `<table>, <tr>,` and `<td>` tags. Here we want to extract any table in the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can get the table\n",
    "full_table = soup.find_all('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A new row in an HTML table starts with <tr> tag\n",
    "# A new column entry is defined by <td> tag\n",
    "table_result = list()\n",
    "for table in full_table:\n",
    "    for row in table.find_all('tr'):\n",
    "        row_cells = row.find_all('td') # find all table data\n",
    "        row_entries = [cell.text for cell in row_cells]\n",
    "        print(row_entries) \n",
    "        table_result.append(row_entries)\n",
    "        # get all the table data into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pandas can also grab tables from a website automatically\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import html5lib\n",
    "# requires html5lib: \n",
    "#!conda install --yes html5\n",
    "dfs = pd.read_html('http://www.comp.hkbu.edu.hk/~hugolee/') \n",
    "# returns a list of all tables at url\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(dfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(dfs)) #list of tables\n",
    "df = pd.concat(dfs,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Looks so-so, however striped from break line characters etc.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make it nicer\n",
    "\n",
    "# Assign column names\n",
    "df.columns=  ['Lab','Detailed Description']\n",
    "\n",
    "# Assing week number\n",
    "weeks = list()\n",
    "for i in range(1,5):\n",
    "    weeks = weeks+['Week{}'.format(i) for tmp in range(2)]\n",
    "df['Week'] = weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Week and Lab as column indices\n",
    "df = df.set_index(['Week','Lab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dropna().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export to excel\n",
    "df.to_excel('labSchedule.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping function to download files of any type from a website\n",
    "\n",
    "Below is a function that takes in a website and a specific file type to download X of them from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extended scraping function of any file format\n",
    "import os # To interact with operating system and format file name\n",
    "import shutil # To copy file object from python to disk\n",
    "import requests\n",
    "import bs4 as bs\n",
    "\n",
    "def py_file_scraper(url, html_tag='img', source_tag='src', file_type='.jpg',max=-1):\n",
    "    \n",
    "    '''\n",
    "    Function that scrapes a website for certain file formats.\n",
    "    The files will be placed in a folder called \"files\" \n",
    "    in the working directory.\n",
    "    \n",
    "    url = the url we want to scrape from\n",
    "    html_tag = the file tag (usually img for images or \n",
    "    a for file links)\n",
    "    \n",
    "    source_tag = the source tag for the file url \n",
    "    (usually src for images or href for files)\n",
    "    \n",
    "    file_type = .png, .jpg, .pdf, .csv, .xls etc.\n",
    "    \n",
    "    max = integer (max number of files to scrape, \n",
    "    if = -1 it will scrape all files)\n",
    "    '''\n",
    "    \n",
    "    # make a directory called 'files' \n",
    "    # for the files if it does not exist\n",
    "    if not os.path.exists('files/'):\n",
    "        os.makedirs('files/')\n",
    "    print('Loading content from the url...')\n",
    "    source = requests.get(url).content\n",
    "    print('Creating content soup...')\n",
    "    soup = bs.BeautifulSoup(source,'html.parser')\n",
    "    \n",
    "    i=0\n",
    "    print('Finding tag:%s...'%html_tag)\n",
    "    for n, link in enumerate(soup.find_all(html_tag)):\n",
    "        file_url=link.get(source_tag)\n",
    "        print ('\\n',n+1,'. File url',file_url)\n",
    "        \n",
    "        \n",
    "        if 'http' in file_url: # check that it is a valid link\n",
    "            print('It is a valid url..')\n",
    "            \n",
    "            \n",
    "            if file_type in file_url: #only check for specific \n",
    "                # file type\n",
    "                \n",
    "                print('%s FILE TYPE FOUND IN THE URL...'%file_type)\n",
    "                file_name = os.path.splitext(os.path.basename(file_url))[0] + file_type \n",
    "                #extract file name from url\n",
    "\n",
    "                file_source = requests.get(file_url, stream = True)\n",
    "             \n",
    "                # open new stream connection\n",
    "\n",
    "                with open('./files/'+file_name, 'wb') as file: \n",
    "                    # open file connection, create file and \n",
    "                    # write to it\n",
    "                    \n",
    "                    shutil.copyfileobj(file_source.raw, file) \n",
    "                    # save the raw file object\n",
    "                    \n",
    "                    print('DOWNLOADED:',file_name)\n",
    "                    \n",
    "                    i+=1\n",
    "                    \n",
    "                del file_source # delete from memory\n",
    "            else:\n",
    "                print('%s file type NOT found in url:'%file_type)\n",
    "                print('EXCLUDED:',file_url) \n",
    "                # urls not downloaded from\n",
    "                \n",
    "        if i == max:\n",
    "            print('Max reached')\n",
    "            break\n",
    "            \n",
    "\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape funny cat pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "py_file_scraper('https://funcatpictures.com/') \n",
    "# scrape cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find out the cat pictures in the folder '/files' under the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape pdf's from Websitesite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "py_file_scraper('---place the url here---',\n",
    "                html_tag='a',source_tag='href',file_type='.pdf', \\\n",
    "                max=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape real data CSV files from websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "py_file_scraper('---place the url here---',\n",
    "                html_tag='a', # R data sets\n",
    "                source_tag='href', file_type='.csv',max=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "\n",
    "In this exercise, you should extract live weather data from:\n",
    "\n",
    "http://forecast.weather.gov/MapClick.php?lat=37.7772&lon=-122.4168\n",
    "\n",
    "* Task scrape\n",
    "    * period / day (as Tonight, Friday, FridayNight etc.)\n",
    "    * the temperature for the period (as Low, High)\n",
    "    * the long weather description (e.g. Partly cloudy, with a low around 49..)\n",
    "    \n",
    "Store the scraped data strings in a Pandas DataFrame\n",
    "\n",
    "\n",
    "\n",
    "**Hint:** The weather information is found in a div tag with `id='seven-day-forecast'`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "page = requests.get(\"http://forecast.weather.gov/MapClick.php?lat=37.7772&lon=-122.4168\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BeautifulSoup4 is a Python library \n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "seven_day = soup.find(id=\"seven-day-forecast\")\n",
    "forecast_items = seven_day.find_all(class_=\"tombstone-container\")\n",
    "tonight = forecast_items[0]\n",
    "print(tonight.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the name of the forecast item, the short description, and the temperature for the first day\n",
    "period = tonight.find(class_=\"period-name\").get_text()\n",
    "short_desc = tonight.find(class_=\"short-desc\").get_text()\n",
    "temp = tonight.find(class_=\"temp\").get_text()\n",
    "\n",
    "print(period)\n",
    "print(short_desc)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = tonight.find(\"img\")\n",
    "desc = img['title']\n",
    "\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use get_text method on each BeautifulSoup object\n",
    "period_tags = seven_day.select(\".tombstone-container .period-name\")\n",
    "periods = [pt.get_text() for pt in period_tags]\n",
    "periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_descs = [sd.get_text() for sd in seven_day.select(\".tombstone-container .short-desc\")]\n",
    "temps = [t.get_text() for t in seven_day.select(\".tombstone-container .temp\")]\n",
    "descs = [d[\"title\"] for d in seven_day.select(\".tombstone-container img\")]\n",
    "\n",
    "print(short_descs)\n",
    "print(temps)\n",
    "print(descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combining our data into a Pandas Dataframe\n",
    "import pandas as pd\n",
    "weather = pd.DataFrame({\n",
    "        \"period\": periods, \n",
    "        \"short_desc\": short_descs, \n",
    "        \"temp\": temps, \n",
    "        \"desc\":descs\n",
    "    })\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can use a regular expression and the Series.str.extract method to pull out the numeric temperature values\n",
    "temp_nums = weather[\"temp\"].str.extract(\"(?P<temp_num>\\d+)\", expand=False)\n",
    "weather[\"temp_num\"] = temp_nums.astype('int')\n",
    "temp_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate mean temperature\n",
    "weather[\"temp_num\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "\n",
    "Starting from https://en.wikipedia.org/wiki/Data_analysis. Then, get all the article links from Data_analysis page. After doing so, iterate this list of articles. For each article link, repeat the above steps. Recursively and stop when the total number of the list of articles exceeds 3000. For these 3000 records, each should contain its own title and the title of another article linked to it. Save the result as a text file.\n",
    "![ex2.png](attachment:ex2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "start_url = 'https://en.wikipedia.org/wiki/Data_analysis'\n",
    "domain = 'https://en.wikipedia.org'\n",
    "\n",
    "''' get soup '''\n",
    "def get_soup(url):\n",
    "    # get contents from url\n",
    "    content = requests.get(url).content\n",
    "    # get soup\n",
    "    return BeautifulSoup(content,'lxml') # choose lxml parser\n",
    "\n",
    "\n",
    "''' return a list of links to other wiki articles '''\n",
    "def extract_articles(url=start_url):\n",
    "    # get soup\n",
    "    soup = get_soup(url)\n",
    "    # find all the paragraph tags\n",
    "    p_tags = soup.findAll('p')\n",
    "    # gather all <a> tags \n",
    "    a_tags = []\n",
    "    for p_tag in p_tags:\n",
    "        a_tags.extend(p_tag.findAll('a'))\n",
    "    # filter the list : remove invalid links\n",
    "    a_tags = [ a_tag for a_tag in a_tags if 'title' in a_tag.attrs and 'href' in a_tag.attrs ]\n",
    "    # get all the article titles\n",
    "    titles = [ a_tag.get('title') for a_tag in a_tags ] \n",
    "    # get all the article links\n",
    "    links  = [ a_tag.get('href')  for a_tag in a_tags ] \n",
    "    # get own title\n",
    "    self_title = soup.find('h1', {'class' : 'firstHeading'}).text\n",
    "    return self_title, titles, links\n",
    "\n",
    "\n",
    "''' main section '''\n",
    "if __name__ == '__main__':\n",
    "    # list of scraped items\n",
    "    items = []\n",
    "    title, ext_titles, ext_links = extract_articles(url=start_url)\n",
    "    items.extend(zip([title]*len(ext_titles), ext_titles))\n",
    "    for ext_link in ext_links:\n",
    "        title, ext_titles, ext_links = extract_articles(domain + ext_link)\n",
    "        items.extend(zip([title]*len(ext_titles), ext_titles))\n",
    "        if len(items) > 3000:\n",
    "            break\n",
    "    # write to file\n",
    "    with open('result.txt','w', encoding='utf-8') as f:\n",
    "        for item in items:\n",
    "            print(item[0] + '->' + item[1] + '\\n')\n",
    "            f.write(item[0] + '->' + item[1] + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
