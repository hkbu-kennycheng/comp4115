{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab02: Techniques in Python for Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Filter out warnings\n",
    "\n",
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot styling\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "plt.rcParams[ 'figure.figsize' ] = 9 , 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define fancy plot to look at distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special distribution plot (will be used later)\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train_loan.csv', index_col=\"Loan_ID\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Boolean Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you do, if you want to filter values of a column based on conditions from another set of columns? For instance, we want a list of all females who are not graduate and got a loan. Boolean indexing can help here. You can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data[\"Gender\"]==\"Female\") & (data[\"Education\"]==\"Not Graduate\") & (data[\"Loan_Status\"]==\"Y\"), [\"Gender\",\"Education\",\"Loan_Status\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Apply Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is one of the commonly used functions for playing with data and creating new variables. Apply returns some value after passing each row/column of a data frame with some function. The function can be both default or user-defined. For instance, here it can be used to find the #missing values in each row and column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new function:\n",
    "def num_missing(x):\n",
    "  return sum(x.isnull())\n",
    "\n",
    "#Applying per column:\n",
    "print ('Missing values per column:')\n",
    "print (data.apply(num_missing, axis=0)) #axis=0 defines that function is to be applied on each column\n",
    "\n",
    "#Applying per row:\n",
    "print ('\\nMissing values per row:')\n",
    "print (data.apply(num_missing, axis=1).head()) #axis=1 defines that function is to be applied on each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we get the desired result.\n",
    "\n",
    "Note: head() function is used in second output because it contains many rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Imputing missing files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"fillna()\" does it in one go. It is used for updating missing values with the overall mean/mode/median of the column. Let's impute the \"Gender\", \"Married\" and \"Self_Employed\" columns with their respective modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we import a function to determine the mode\n",
    "from scipy.stats import mode\n",
    "mode(data['Gender'].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns both mode and count. Remember that mode can be an array as there can be multiple values with high frequency. We will take the first one by default always using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode(data['Gender'].dropna()).mode[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fill the missing values and check using technique 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the values:\n",
    "data['Gender'].fillna(mode(data['Gender'].dropna()).mode[0], inplace=True)\n",
    "data['Married'].fillna(mode(data['Married'].dropna()).mode[0], inplace=True)\n",
    "data['Self_Employed'].fillna(mode(data['Self_Employed'].dropna()).mode[0], inplace=True)\n",
    "\n",
    "#Now check the #missing values again to confirm:\n",
    "print (data.apply(num_missing, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, it is confirmed that missing values are imputed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pivot Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can be used to create MS Excel style pivot tables. For instance, in this case, a key column is \"LoanAmount\" which has missing values. We can impute it using mean amount of each 'Gender', 'Married' and 'Self_Employed' group. The mean 'LoanAmount' of each group can be determined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine pivot table\n",
    "impute_grps = data.pivot_table(values=[\"LoanAmount\"], index=[\"Gender\",\"Married\",\"Self_Employed\"], aggfunc=np.mean)\n",
    "print (impute_grps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Multi-Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you notice the output of 3, it has a strange property. Each index is made up of a combination of 3 values. This is called Multi-Indexing. It helps in performing operations really fast.\n",
    "\n",
    "Continuing the example from 3, we have the values for each group but they have not been imputed.\n",
    "This can be done using the various techniques learned till now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate only through rows with missing LoanAmount\n",
    "for i,row in data.loc[data['LoanAmount'].isnull(),:].iterrows():\n",
    "  ind = tuple([row['Gender'],row['Married'],row['Self_Employed']])\n",
    "  data.loc[i,'LoanAmount'] = impute_grps.loc[ind].values[0]\n",
    "\n",
    "#Now check the #missing values again to confirm:\n",
    "print (data.apply(num_missing, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to get an initial view of the data. Here, we can validate some basic hypothesis. For instance, in this case, \"Credit_History\" is expected to affect the loan status significantly. This can be tested using cross-tabulation as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data[\"Credit_History\"],data[\"Loan_Status\"],margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are absolute numbers. But, percentages can be more intuitive in making some quick insights. We can do this using the apply function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percConvert(ser):\n",
    "  return ser/float(ser[-1])\n",
    "pd.crosstab(data[\"Credit_History\"],data[\"Loan_Status\"],margins=True).apply(percConvert, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Merge DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging dataframes become essential when we have information coming from different sources to be collated. Consider a hypothetical case where the average property rates (INR per sq meters) is available for different property types. Let's define a dataframe as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_rates = pd.DataFrame([1000, 5000, 12000], index=['Rural','Semiurban','Urban'],columns=['rates'])\n",
    "prop_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can merge this information with the original dataframe as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged = data.merge(right=prop_rates, how='inner',left_on='Property_Area',right_index=True, sort=False)\n",
    "data_merged.pivot_table(values='Credit_History',index=['Property_Area','rates'], aggfunc=len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Sorting DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting can be based on multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted = data.sort_values(['ApplicantIncome','CoapplicantIncome'], ascending=False)\n",
    "data_sorted[['ApplicantIncome','CoapplicantIncome']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Plotting (Boxplot & Histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplots and Histograms can be directly plotted in Pandas and calling matplotlib separately is not necessary. For instance, if we want to compare the distribution of CoapplicantIncome by Loan_Status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "data.boxplot(column=\"CoapplicantIncome\",by=\"Loan_Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(column=\"ApplicantIncome\",by=\"Loan_Status\",bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Cut function for binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes numerical values make more sense if clustered together. For example, if we're trying to model traffic (#cars on road) with time of the day (minutes). The exact minute of an hour might not be that relevant for predicting traffic as compared to actual period of the day like \"Morning\", \"Afternoon\", \"Evening\", \"Night\", \"Late Night\". Modeling traffic this way will be more intuitive and will avoid overfitting.\n",
    "\n",
    "Here we define a simple function which can be re-used for binning any variable fairly easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binning:\n",
    "def binning(col, cut_points, labels=None):\n",
    "  #Define min and max values:\n",
    "  minval = col.min()\n",
    "  maxval = col.max()\n",
    "\n",
    "  #create list by adding min and max to cut_points\n",
    "  break_points = [minval] + cut_points + [maxval]\n",
    "\n",
    "  #if no labels provided, use default labels 0 ... (n-1)\n",
    "  if not labels:\n",
    "    labels = range(len(cut_points)+1)\n",
    "\n",
    "  #Binning using cut function of pandas\n",
    "  colBin = pd.cut(col,bins=break_points,labels=labels,include_lowest=True)\n",
    "  return colBin\n",
    "\n",
    "#Binning age:\n",
    "cut_points = [90,140,190]\n",
    "labels = [\"low\",\"medium\",\"high\",\"very high\"]\n",
    "data[\"LoanAmount_Bin\"] = binning(data[\"LoanAmount\"], cut_points, labels)\n",
    "print (pd.value_counts(data[\"LoanAmount_Bin\"], sort=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Coding nominal data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we find a case where we've to modify the categories of a nominal variable. This can be due to various reasons:\n",
    "\n",
    "1. Some algorithms (like Logistic Regression) require all inputs to be numeric. So nominal variables are mostly coded as 0, 1….(n-1)\n",
    "2. Sometimes a category might be represented in 2 ways. For e.g. temperature might be recorded as \"High\", \"Medium\", \"Low\", \"H\", \"low\". Here, both \"High\" and \"H\" refer to same category. Similarly, in \"Low\" and \"low\" there is only a difference of case. But, python would read them as different levels.\n",
    "3. Some categories might have very low frequencies and its generally a good idea to combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a generic function using Pandas replace function\n",
    "def coding(col, codeDict):\n",
    "  colCoded = pd.Series(col, copy=True)\n",
    "  for key, value in codeDict.items():\n",
    "    colCoded.replace(key, value, inplace=True)\n",
    "  return colCoded\n",
    " \n",
    "#Coding LoanStatus as Y=1, N=0:\n",
    "print ('Before Coding:')\n",
    "print (pd.value_counts(data[\"Loan_Status\"]))\n",
    "data[\"Loan_Status_Coded\"] = coding(data[\"Loan_Status\"], {'N':0,'Y':1})\n",
    "print ('\\nAfter Coding:')\n",
    "print (pd.value_counts(data[\"Loan_Status_Coded\"]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
